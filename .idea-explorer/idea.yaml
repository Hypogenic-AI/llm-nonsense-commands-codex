idea:
  title: Do LLMs Understand Nonsense Commands?
  domain: nlp
  hypothesis: 'Large language models may not be able to explain or interpret prompts
    that are optimized to produce outputs with high perplexity or outputs that do
    not resemble English, suggesting that directed prompt-based "jailbreaking" is
    difficult and fundamentally different from standard prompt understanding.

    '
  background:
    description: 'A simple idea:

      Find a prompt that does something specific, for instance, causes the model to
      write a poem that doesn''t look anything like English. Use prompt optimization
      with a specific focus on increasing the perplexity of prompt. Ask the model
      to explain its prompt. Study. Will models be able to explain prompts the way
      they can English? Or are these prompts hard to find and jailbreaking is just
      a special case, but a directed jailbreak is hard to find?'
  metadata:
    source: IdeaHub
    source_url: https://hypogenic.ai/ideahub/idea/QnPYsHR9aQDCkAc64amN
    idea_id: do_llms_understand_nonsense_co_20251214_191417_07e775dd
    created_at: '2025-12-14T19:14:17.264072'
    status: submitted
    github_repo_name: llm-nonsense-commands-codex
    github_repo_url: https://github.com/Hypogenic-AI/llm-nonsense-commands-codex
